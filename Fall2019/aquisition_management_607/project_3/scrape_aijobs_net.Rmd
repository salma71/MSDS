---
title: "Best Data scientist skills analysis from ai-job.net"
author: "Salma Elshahawy"
date: "10/11/2019"
output: 
  html_document:
    rmarkdown::html_document:
    code_folding: show
    df_print: paged
    highlight: pygments
    number_sections: no
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading necessary libraries

```{r loading package, message=FALSE, warning=FALSE}
library(stringr)
library(dplyr)
library(wordcloud)
library(tm)
library(RCurl)
library(XML)
library(kableExtra)
library(rvest)
library(stringr)
library(tidyverse)
library(purrr)
library(here)
library(beepr)
library(DT)
```

## Introduction

It is required to scrape a job-board website to extract the most required skills for Data Scientist. I decided to scrape [ai-jobs](https://www.ai-jobs.net) which is a job board that specifically serves the artificial intelligence (AI) and data science community. Whether you’re looking for something permanent or a contract role, there’s an option for everyone. The jobs listed on the website can also be found on their

## Convert javascript rendered HTML into HTML only

ai-jobs uses javascript language to render the HTML page using asynchronous request. If you use an asynchronous XMLHttpRequest, you receive a callback when the data has been received. This lets the browser continue to work as normal while your request is being handled.

rvest and httr doesn't work for that site, so I had to write javascript code insode of R to be able to scrape the page.I wrote the JavaScript code to a js file called scrape.js using *Phantomjs.*.I used PhantomJS to convert the website into HTML. I wrapped this in a system() call inside R Studio.

```{r message=FALSE, warning=FALSE}

writeLines("const url = 'https://ai-jobs.net/';
const page = new WebPage();
const fs = require('fs');

page.open(url, function (status) {
        just_wait();
});

function just_wait() {
    setTimeout(function() {
               fs.write('scrape_ai.html', page.content, 'w');
            phantom.exit();
    }, 2500);
}
", con = "scrape_ai.js")

js_scrape <- function(url = "https://ai-jobs.net/", 
                      js_path = "scrape_ai.js", 
                      phantompath = "/usr/local/bin/phantomjs"){
  
  lines <- readLines(js_path)
  lines[1] <- paste0("const url ='", url ,"';")
  writeLines(lines, js_path)
  
  command = paste(phantompath, js_path, sep = " ")
  system(command)

}

js_scrape()

## now we can read the newly html with the job summary from the new html after converting from js 
summary_html <- read_html("scrape_ai.html")
summary_html
```


## Pulling down the data 

I constructed a matrix as a data frame structured in 5 columns to hold the pulled data.

```{r message = FALSE, warning = FALSE}
jobs_data <- data.frame(matrix(ncol = 5, nrow = 0))
```


```{r message = FALSE, warning = FALSE}
  
  #position name
  get_position <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".position") %>% 
    html_text() %>%
  # get only position names
    gsub("[\n\t]", "", .) %>%
    stringr::str_trim() %>%
    strsplit("  ") %>% 
    unlist() 
get_position
```

```{r message = FALSE, warning = FALSE}
  # get the company name
  get_company <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".company") %>% 
    html_text() %>% 
    gsub("[\n\t]", " ", .) %>% 
    stringr::str_trim() %>% 
    unlist()
get_company
```

```{r message = FALSE, warning = FALSE}
  # get the position - location
  get_location <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".location") %>% 
    html_text() %>% 
    gsub("[\n\t]", " ", .) %>% 
    stringr::str_trim() %>% 
    unlist()  
get_location
```

```{r message = FALSE, warning = FALSE}
  # get detailed url 
  job_details <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes(".job_listings") %>% 
    html_nodes("li") %>% 
    html_nodes("a") %>% 
    html_attr("href") 
```

For the job full description, I got the link for each job posting **(href attribute)** and iterate over those hrefs and scrape them to get the full description information. 

```{r message = FALSE, warning = FALSE}
  # get detailed description 
  get_description <- as.character()
    for ( n in 1:length(job_details) ){
      #build the link
      link <- job_details[n]
      #pull the link
      page <- read_html(link)
      #get the full summary
      get_description[n] <- read_html(job_details[n]) %>%
       html_nodes("div") %>% 
       html_nodes(xpath = '//*[@id="primary"]/article/div/div/div[2]') %>% 
       html_text(trim = TRUE)
      
      }
  get_description[1]
```


## Final Dataframe

```{r message = FALSE, warning = FALSE}
  jobs_data <- rbind(jobs_data,data.frame(get_position,
                                            get_company,
                                            get_location,
                                            job_details,
                                            get_description))
jobs_data %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
```

```{r message = FALSE, warning = FALSE}
dim(jobs_data)
```

## Exporting to csv file

```{r message = FALSE, warning = FALSE}
# write the resulted data into a csv table ---> transformation 
job_scrape_df <- write.table(jobs_data, file = "jobs_detailsInfo.csv", row.names = FALSE, na="", col.names = TRUE, sep = ",")
```

## Test that the file is already created. 

*Please be sure that you set working directory in Rstudio to the current working directory.*

```{r message = FALSE, warning = FALSE}
file_test("-f", "~/Desktop/MSDS_2019/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv")
```

## open the file from local machine

```{r message = FALSE, warning = FALSE}
job_details_df <- read.csv(file = "jobs_detailsInfo.csv", header = TRUE, sep = ",")

head(job_details_df, 2)   %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```

## Open the file from GitHub account

```{r message = FALSE, warning = FALSE}
url <- "https://raw.githubusercontent.com/salma71/MSDS_2019/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv"

jobs_df <- read.csv(url, header = TRUE, sep = ",", stringsAsFactors = FALSE)
jobs_df
```

_________

## Important links 

+ [GitHub link for the final csv file](https://github.com/salma71/MSDS_2019/blob/master/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv)

+ [GitHub link for the RMD file](https://github.com/salma71/MSDS_2019/tree/master/Fall2019/aquisition_management_607/project_3)

+ [Rpubs link](http://rpubs.com/salmaeng/ds_skills_607)

+ [ai-jobs.net](https://www.ai-jobs.net)

__________

## Extra text analysis using bag of words

we need to change the get_details column into a corpus for analysis

```{r message = FALSE, warning = FALSE}

job_desc_corpus <- VCorpus(VectorSource(job_details_df$get_description))

#summary(job_desc_corpus)


```

also used regex to remove ASCII characters

```{r message = FALSE, warning = FALSE}
for (j in seq(job_desc_corpus)) {
    job_desc_corpus[[j]] <- gsub("/", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("@", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("\\|", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("\u2028", "  ", job_desc_corpus[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(job_desc_corpus[[1]]))
```


```{r message = FALSE, warning = FALSE}
# remove puncituation 
job_desc <- tm_map(job_desc_corpus,removePunctuation) 
writeLines(as.character(job_desc[[1]]))

# remove numbers
job_desc <- tm_map(job_desc,removeNumbers) 
# remove ascii characters
writeLines(as.character(job_desc[[1]]))

```

```{r message = FALSE, warning = FALSE}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
# stopwords("english")   
job_desc <- tm_map(job_desc, removeWords, stopwords("english"))   
job_desc <- tm_map(job_desc, PlainTextDocument)
#writeLines(as.character(job_desc[[1]]))

```
```{r message = FALSE, warning = FALSE}
job_desc <- tm_map(job_desc, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

Removing common word endings (e.g., “ing”, “es”, “s”)
This is referred to as “stemming” documents. We stem the documents so that a word will be recognizable to the computer, despite whether or not it may have a variety of possible endings in the original text.

Note: The “stem completion” function is currently problemmatic, and stemmed words are often annoying to read. For now, I have this section commented out. But you are welcome to try these functions (by removing the hashmark from the beginning of the line) if they interest you. Just don’t expect them to operate smoothly.

This procedure has been a little hanky in the recent past, so I change the name of the data object when I do this to keep from overwriting what I have done to this point.

```{r message = FALSE, warning = FALSE}
jobs_st <- tm_map(job_desc, stemDocument)   
jobs_st <- tm_map(job_desc, PlainTextDocument)
writeLines(as.character(jobs_st[1])) # Check to see if it worked.
```















