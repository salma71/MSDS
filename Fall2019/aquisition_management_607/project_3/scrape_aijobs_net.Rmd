---
title: "Untitled"
author: "Salma Elshahawy"
date: "10/11/2019"
output: 
  html_document:
    rmarkdown::html_document:
    code_folding: show
    df_print: paged
    highlight: pygments
    number_sections: no
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading package, message=FALSE, warning=FALSE}
library(stringr)
library(dplyr)
library(wordcloud)
library(tm)
library(RCurl)
library(XML)
library(kableExtra)
library(rvest)
library(stringr)
library(tidyverse)
library(purrr)
library(here)
library(beepr)
library(DT)
```



```{r message=FALSE, warning=FALSE}

writeLines("var url = 'https://ai-jobs.net/';
var page = new WebPage();
var fs = require('fs');

page.open(url, function (status) {
        just_wait();
});

function just_wait() {
    setTimeout(function() {
               fs.write('scrape_ai.html', page.content, 'w');
            phantom.exit();
    }, 2500);
}
", con = "scrape_ai.js")

js_scrape <- function(url = "https://ai-jobs.net/", 
                      js_path = "scrape_ai.js", 
                      phantompath = "/usr/local/bin/phantomjs"){
  
  # this section will replace the url in scrape.js to whatever you want 
  lines <- readLines(js_path)
  lines[1] <- paste0("var url ='", url ,"';")
  writeLines(lines, js_path)
  
  command = paste(phantompath, js_path, sep = " ")
  system(command)

}

js_scrape()

## now we can read the newly html with the job summary from the new html after converting from js 
summary_html <- read_html("scrape_ai.html")
summary_html
```


cleaning up the string from special characters


```{r}
jobs_data <- data.frame(matrix(ncol = 5, nrow = 0))
n <- c("position","company","location","details_link","details")
colnames(jobs_data)
```


```{r}
  
  #position name
  get_position <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".position") %>% 
    html_text() %>%
  # get only position names
    gsub("[\n\t]", "", .) %>%
    stringr::str_trim() %>%
    strsplit("  ") %>% 
    unlist() 
get_position
```

```{r}
  # get the company name
  get_company <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".company") %>% 
    html_text() %>% 
    gsub("[\n\t]", " ", .) %>% 
    stringr::str_trim() %>% 
    unlist()
get_company
```

```{r}
  # get the position - location
  get_location <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes("ul") %>% 
    html_nodes(".location") %>% 
    html_text() %>% 
    gsub("[\n\t]", " ", .) %>% 
    stringr::str_trim() %>% 
    unlist()  
get_location
```

```{r}
  # get detailed url 
  job_details <- summary_html %>% 
    html_nodes("div") %>% 
    html_nodes(".job_listings") %>% 
    html_nodes("li") %>% 
    html_nodes("a") %>% 
    html_attr("href") 
```

```{r}
  # get detailed description 
  get_description <- as.character()
    for ( n in 1:length(job_details) ){
      #build the link
      link <- job_details[n]
      #pull the link
      page <- read_html(link)
      #get the full summary
      get_description[n] <- read_html(job_details[n]) %>%
       html_nodes("div") %>% 
       html_nodes(xpath = '//*[@id="primary"]/article/div/div/div[2]') %>% 
       html_text(trim = TRUE)
      
      }
  get_description
```



```{r}
#fill in the job data #create a structure to hold our full summaries
  position <- rep(get_position,length(job_details))
  company <- rep(get_company,length(job_details))
  location <- rep(get_location,length(job_details))
  details_link <- rep(job_details,length(job_details))
  details <- rep(get_description, length(job_details))
```

```{r}
  jobs_data <- rbind(jobs_data,data.frame(get_position,
                                            get_company,
                                            get_location,
                                            job_details,
                                            get_description))
jobs_data %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
```

```{r}
dim(jobs_data)
```


```{r}
# write the resulted data into a csv table ---> transformation 
job_scrape_df <- write.table(jobs_data, file = "jobs_detailsInfo.csv", row.names = FALSE, na="", col.names = TRUE, sep = ",")
```

## Test that the file is already created. 

*Please be sure that you set working directory in Rstudio to the current working directory.*

```{r}
file_test("-f", "~/Desktop/MSDS_2019/Fall2019/aquisition_management_607/project_3/jobs_detailsInfo.csv")
```

## open the file from local machine

```{r}
library(readr)
job_details_df <- read.csv(file = "jobs_detailsInfo.csv", header = TRUE, sep = ",")

head(job_details_df, 2)   %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```

we need to change the get_details column into a corpus for analysis

```{r}

job_desc_corpus <- VCorpus(VectorSource(job_details_df$get_description))

#summary(job_desc_corpus)


```

also used regex to remove ASCII characters

```{r}
for (j in seq(job_desc_corpus)) {
    job_desc_corpus[[j]] <- gsub("/", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("@", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("\\|", "  ", job_desc_corpus[[j]])
    job_desc_corpus[[j]] <- gsub("\u2028", "  ", job_desc_corpus[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(job_desc_corpus[[1]]))
```


```{r}
# remove puncituation 
job_desc <- tm_map(job_desc_corpus,removePunctuation) 
writeLines(as.character(job_desc[[1]]))

# remove numbers
job_desc <- tm_map(job_desc_corpus,removeNumbers) 
# remove ascii characters
#writeLines(as.character(job_desc[[1]]))

```

```{r}
# For a list of the stopwords, see:   
# length(stopwords("english"))   
# stopwords("english")   
job_desc <- tm_map(job_desc, removeWords, stopwords("english"))   
job_desc <- tm_map(job_desc, PlainTextDocument)
#writeLines(as.character(job_desc[[1]]))

```
```{r}
job_desc <- tm_map(job_desc, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

Removing common word endings (e.g., “ing”, “es”, “s”)
This is referred to as “stemming” documents. We stem the documents so that a word will be recognizable to the computer, despite whether or not it may have a variety of possible endings in the original text.

Note: The “stem completion” function is currently problemmatic, and stemmed words are often annoying to read. For now, I have this section commented out. But you are welcome to try these functions (by removing the hashmark from the beginning of the line) if they interest you. Just don’t expect them to operate smoothly.

This procedure has been a little hanky in the recent past, so I change the name of the data object when I do this to keep from overwriting what I have done to this point.

```{r}
jobs_st <- tm_map(jobs_desc, stemDocument)   
jobs_st <- tm_map(jobs_desc, PlainTextDocument)
writeLines(as.character(jobs_st[1])) # Check to see if it worked.
```















