---
title: "Kaggle_job_search"
author: "Salma Elshahawy"
date: "10/10/2019"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load the required package

```{r loading package, message=FALSE, warning=FALSE}
library(stringr)
library(dplyr)
library(wordcloud)
library(tm)
library(RCurl)
library(XML)
```

## Scrape the links from Kaggle

Next step, we need to extract the job postings links from Kaggle.

Here I will follow the more traditional text scrapping approach using corpus and document-term matrix. It consists of querying a job hosting site for _Data Science jobs_ and obtaining a list of URL each representing an individual job posting, iterating over this list and retrieving the individual job posting, parsing the body of each job posting identifying keys words (such as ‘required skills’, ‘Requirements’, .) and scraping the text and storing each extract into a corpus. Then the corpus is then processed to convert everything to lower-case, remove all punctuation, strip extra white-space, and finally remove “stop-words” of the English language (prepositions, articles, possessive, …). Finally, a document-term matrix is created. This approach makes use of the TM package. The raw data is then filter to remove “noise” and irrelevant terms. The result set is then stored in .csv file.

```{r}
#https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20scientist&locT=&locId=0&locKeyword=
# This function to creates a URL for the www.dice.com website and extract the data 
create_url <- function(website, title, location, locatId, page){
  url <- paste0(website, "?sc.keyword=", title, "&locT=", location, "&locId=", locatId)
  url <- paste0(url, "&startPage=", page, "&locKeyword=")
  return(url)
}

# This function use the unstructure data from the html file to create a dataframe
# with only the data that is needed for analysis
create_tibble <- function(html){
  
  search_title <- html %>% 
    html_nodes(".complete-serp-result-div") %>%
    html_nodes(xpath = "//a/@title") %>%
    html_text()
  
  search_region <- html %>% 
    html_nodes(".complete-serp-result-div") %>% 
    html_nodes('[itemprop="address"]') %>%
    html_nodes("[itemprop=addressRegion]") %>%
    html_text()
  
  search_zipcode <- html %>% 
    html_nodes(".complete-serp-result-div") %>% 
    html_nodes('[itemprop="address"]') %>%
    html_nodes("[itemprop=postalCode]") %>%
    html_text()
  
  search_address <- html %>% 
    html_nodes(".complete-serp-result-div") %>% 
    html_nodes('[itemprop="address"]') %>%
    html_nodes("[itemprop=streetAddress]") %>%
    html_text() %>% 
    str_replace(pattern = paste0(", ",search_region), "")
  
  search_company <- html %>% 
    html_nodes("div") %>% 
    html_nodes('.jobEmpolyerName') %>%
    html_text()
  
    df <- tibble(title = search_title,
                 company = search_company,
                 city = search_address, 
                 state = search_region,
                 zipcode = search_zipcode)

    return(df)
}
```

```{r}
create_tibble <- function(html){
  
  search_title <- html %>% 
    html_nodes("div") %>%
    html_nodes("a") %>%
    html_nodes(".jobTitle")
    html_text()
    
  search_company <- html %>% 
    html_nodes("div") %>% 
    html_nodes('.jobEmpolyerName') %>%
    html_text()
  
  
  df <- tibble(title = search_title,
                 company = search_company)

    return(df)
}
  
```


```{r}
site = "https://www.dice.com/jobs"
job = "Data+Science"
region = "MI"
miles = 30
pag = 1

#url <- create_url(website = site, title = JOB_TITLE, location = STATE_TWO_LETTERS, radius = NUM_MILES, page = PAG_NUM)
url <- create_url(website = site, title = job, location = region, radius = miles, page = pag)
#url
url
```

```{r}
knitr::include_graphics('imgs/datascience.png')
```

```{r}
#site = JOB_WEBSITE
#job = "Data+Analyst"
#region = STATE_TWO_LETTERS
#miles = MILES_NUM
#pag = PAG_NUM
num_pages = 29

# COMMENT: Loop over the max number of pages for the job search
  for (i in 1:num_pages) {

  # TODO: Create a url for the job search
  #url <- create_url()
  url <- create_url(website = site, title = job, location = region, radius = miles, page = num_pages)
  # COMMENT: read the created URL and collects the html code
  web_html <- read_html(url)

  # COMMENT: If statement to create the first dataframe
  if(i == 1) {
    
    # COMMENT: Creates a tibble dataframe extracting information from the html code
    job_data <- create_tibble(html = web_html)
    
  }else{
  
      # COMMENT: We add new observation to the first dataframe
    df <- create_tibble(html = web_html)
    job_data <- bind_rows(job_data, df)
  }
  
  # COMMENT: We have to wait a couple of seconds before moving to the next page
  Sys.sleep(1.0)
}
```

```{r}
summary(job_data)
```

```{r}
#Loading the rvest package
library('rvest')

#Specifying the url for desired website to be scraped
url <- 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=data%20scientist&locT=&locId=0&locKeyword='

#Reading the HTML code from the website
webpage <- read_html(url)

webpage %>% 
  html_nodes("div") %>%
  html_nodes(".jobEmpolyerName") %>%
  html_text() # employer name

webpageee %>% html_nodes(".loc")%>%html_text()  # job location

webpageee %>% html_nodes(".salaryText")%>%html_text()  ## salary

webpageee %>% html_nodes(".jobTitle")%>%html_attr("href") ## getting href for each posting - need to extract id from the link 

new_url <- parse_url("/partner/jobListing.htm?pos=128&ao=787889&s=58&guid=0000016db87c49c7ae5177c6534fada7&src=GD_JOB_AD&t=SR&extid=1&exst=O&ist=L&ast=OL&vt=w&slr=true&cs=1_b54298d6&cb=1570758217052&jobListingId=3379135896")

job_id_jl <- read_html(url) %>% html_nodes(".jl") %>% html_attr("data-id") %>% as.character() ## extracting ids attributes 

data_job_loc_id_jv <- read_html(url) %>% html_nodes(".jl") %>% html_attr("data-job-loc-id") %>% as.character()

title <- read_html(url) %>% html_nodes(".jl") %>% html_attr("data-normalize-job-title") %>% as.character()

job_id <- new_url$query$jobListingId ## extracting only the id 
##[1] "3379135896"

#https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=data+scientist&sc.keyword=data+scientist&locT=C&locId=1132348&jobType=

#https://www.glassdoor.com/job-listing/data-scientist-customer-operations-squarespace-JV_IC1132348_KO0,34_KE35,46.htm?jl=3334367086&ctt=1570794789074
#job_desc <- 
#rank_data_html <- html_nodes(webpage,'.jobEmpolyerName')

#Converting the ranking data to text
#rank_data <- html_text(rank_data_html)

#Let's have a look at the rankings
#head(rank_data)
```

```{r}
URLs2 <- as.character()
for (i in 1:length(webpageee %>% html_nodes(".jobTitle")%>%html_attr("href"))) {
  URLs2[i]<-as.character(webpageee %>% html_nodes(".jobTitle")%>%html_attr("href"))[i]
}
```

```{r}
library(httr)
res <- GET(url = 'https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm', add_headers(
    Referer = "https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm", 
    `X-Requested-With` = "XMLHttpRequest"
  ))
res

webpage <- read_html(res)
webpage %>%
  html_nodes("div") %>%
  html_nodes("a") %>%
  html_attr("href") 
```

```{r}
system("/usr/local/bin/phantomjs scrape_glassdoor.js")

withJS <- xml2::read_html("glassdoor_scrape.html") %>% rvest::html_nodes(".descSnippet") %>% rvest::html_text()
withJS
```

```{r}
library(rvest)
library(stringr)
library(tidyverse)
library(purrr)
library(here)
library(beepr)
library(DT)
writeLines("var url = 'https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm';
var page = new WebPage();
var fs = require('fs');

page.open(url, function (status) {
        just_wait();
});

function just_wait() {
    setTimeout(function() {
               fs.write('1.html', page.content, 'w');
            phantom.exit();
    }, 2500);
}
", con = "scrape.js")

js_scrape <- function(url = "https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm", 
                      js_path = "scrape.js", 
                      phantompath = "/usr/local/bin/phantomjs"){
  
  # this section will replace the url in scrape.js to whatever you want 
  lines <- readLines(js_path)
  lines[1] <- paste0("var url ='", url ,"';")
  writeLines(lines, js_path)
  
  command = paste(phantompath, js_path, sep = " ")
  system(command)

}

js_scrape()

html <- read_html("1.html")
setup <- html %>% html_nodes(".descSnippet") %>% html_text()
setup
```

```{r}
tab <- as.character()
web <- webpage %>%
    html_nodes("div") %>%
    html_nodes("a") %>%
    html_attr("href") %>% as.character()
for (i in 1:length(web)) {
  tab[i] <- web[i]
}
tab
```

```{r}
url_1 <- "https://www.glassdoor.com/job-listing/data-scientist-henkel-JV_IC1148424_KO0,14_KE15,21.htm?jl=3350343247&ctt=1570810271724"
read_html(url_1)
read_html(url_1) %>% html_nodes("div") %>% html_nodes(".desc") %>% html_text()
## getting the full description
```






