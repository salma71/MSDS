---
title: "DATA607_Project 4_Document classification"
author: "Salma Elshahawy"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment statement

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). [One example corpus:](https://spamassassin.apache.org/old/publiccorpus/)

## Introduction 

A common task in social science involves hand-labeling sets of documents for specific variables (e.g. manual coding). In previous years, this required hiring a set of research assistants and training them to read and evaluate text by hand. It was expensive, prone to error, required extensive data quality checks, and was infeasible if you had an extremely large corpus of text that required classification.

Alternatively, we can now use statistical learning models to classify text into specific sets of categories. This is known as supervised learning. The basic process is:

+ Hand-code a small set of documents (say 1000) for whatever variable(s) you care about

+ Train a statistical learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors

+ Evaluate the effectiveness of the statistical learning model via cross-validation

+ Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (say 1000000)

## Loading necessary libraries

```{r message=FALSE, warning=FALSE}
library(R.utils)
library(tidyverse)
library(tidytext)
library(readtext)
library(stringr)
library(tm)
```

## Download the dataset into local machine


```{r}
base_url_spam <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"
spam_zip <- "20030228_spam_2.tar.bz2"
spam_tar <- "20030228_spam_2.tar"

base_url_ham <- "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2"
ham_zip <- "20030228_easy_ham_2.tar.bz2"
ham_tar <- "20030228_easy_ham_2.tar"
  if(!file.exists(spam_tar)){
    res_spam <- tryCatch(download.file(base_url_spam,
                                  destfile= spam_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(spam_zip)
    untar(spam_tar, exdir="spam_ham_documents")
  } 

  if(!file.exists(ham_tar)){
    res_ham <- tryCatch(download.file(base_url_ham,
                                  destfile= ham_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(ham_zip)
    untar(ham_tar, exdir = "spam_ham_documents")
    
  } else {
    paste("The file is already exists!")
  }
```

## Read content of each file


```{r}
base_dir <- "/Users/salmaelshahawy/Desktop/MSDS_2019/Fall2019/aquisition_management_607/week_10/spam_ham_documents"

email_content <- NA

get_content <- function(type) {
  files_path <- paste(base_dir,type, sep = "/")
  files_name <- list.files(files_path)
    for (file in 1:length(files_name)) {
      file_path <- paste(files_path, files_name[file], sep = "/")
      content_per_file <- file_path %>%
        lapply(readLines)
      
      email_content <- c(email_content, content_per_file)
    }
  return(email_content)
}
```

```{r}
spam_test <- get_content("spam_2") #list
ham_test <- get_content("easy_ham_2") #list
```


```{r}
get_nested_content <- function(list_name) {
  nested_value <- NA
  for (value in 2:length(list_name)) {
    value_per_row <- list_name[[value]]
    nested_value <- c(nested_value, value_per_row)
  }
  return(nested_value)
}
```

```{r}
spam_content <- get_nested_content(spam_test)
ham_content <- get_nested_content(ham_test)
```

## create a datafram for the nested list

```{r}
make_df <- function(list_name, type) {
    df <- data.frame(stringsAsFactors = FALSE)
    df <- rbind(df, list_name) %>%
      mutate(class = type) 
    names(df)[1] <- "texts"
}


```

```{r}
spam_df <- as.data.frame(spam_content) %>%
  mutate(class = "spam") %>% #adding a class tag 
  na.omit(spam_df)
names(spam_df) <- c("text", "class")
spam_df
```
```{r}
ham_df <- as.data.frame(ham_content) %>%
  mutate(class = "ham") %>% #adding a class tag 
  na.omit(ham_df) 
names(ham_df) <- c("text", "class")
ham_df
```

```{r}
data_df <- rbind(spam_df, ham_df) %>%
  mutate_all(funs(gsub("[^[:alnum:][:blank:]+\\s+?&-]", "",.)))
data_df 
```

```{r}
table(data_df$class)
```

## Converting into corpus

Convert the resulting documents from downloading into corpus. To do so, I wrote a function as the following 

```{r}
corpus_data = VCorpus(VectorSource(data_df$text))
```

## cleaning up

```{r}
corpus_data = tm_map(corpus_data, content_transformer(stringi::stri_trans_tolower))
corpus_data = tm_map(corpus_data, removeNumbers)
corpus_data = tm_map(corpus_data, removePunctuation)
corpus_data = tm_map(corpus_data, stripWhitespace)
corpus_data = tm_map(corpus_data, removeWords, stopwords("english"))
corpus_data = tm_map(corpus_data, stemDocument)

#as.character(corpus[[1]])
```

## Creating bag of words using Document tearm matrix

```{r}
dtm <- DocumentTermMatrix(corpus_data)
dtm
```









## Create a lable for spam/ham document category label

```{r}
add_tag <- function(corpus, tag, value){
  for (i in 1:length(corpus)){
    meta(corpus[[i]], tag) <- value                    # Add the value to the specified tag
  }
  return(corpus)
}
```

```{r}
to_corpus <- function(doc, type) {
  corpus_name <- VCorpus(VectorSource(data_df)) %>%
    add_tag(tag = "document type", value = type)
  return(corpus_name)
}
```


```{r}
spam_corpus <- to_corpus(spam_test,"spam")
#summary(spam_corpus)
#str(spam_corpus)
ham_corpus <- to_corpus(ham_test, "ham")
#summary(ham_corpus)
```

## Clean the corpus using **tm** package

```{r}
get_clean <- function(corpus_name){
  cleaned_corpus <- corpus_name %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(content_transformer(stringi::stri_trans_tolower)) %>%
    tm_map(PlainTextDocument)
  
  return(cleaned_corpus)
}
```

```{r} 
cleaned_spam <- get_clean(spam_corpus)
```




```{r}
# for the spam folder
spam_corpus <- VCorpus(DirSource(spam_df))
#summary(spam_corpus)
```
##############

# for the ham folder
ham_files <- list.files("././spam_ham_documents/easy_ham_2")
ham_path <- file.path("././spam_ham_documents/easy_ham_2")
ham_corpus <- VCorpus(DirSource(ham_path))
#summary(spam_corpus)

```

## Cleaning up the corpus using the tm package

For this purpose, I constructed a function that takes the corpus as an argument and use **tm** package from R to clean up the corpus.
