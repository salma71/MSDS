---
title: "DATA607_Project 4_Document classification"
author: "Salma Elshahawy"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
 knitr::opts_chunk$set(echo = TRUE)
```

## Assignment statement

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). [One example corpus:](https://spamassassin.apache.org/old/publiccorpus/)

## Introduction 

A common task in social science involves hand-labeling sets of documents for specific variables (e.g. manual coding). In previous years, this required hiring a set of research assistants and training them to read and evaluate text by hand. It was expensive, prone to error, required extensive data quality checks, and was infeasible if you had an extremely large corpus of text that required classification.

Alternatively, we can now use statistical learning models to classify text into specific sets of categories. This is known as supervised learning. The basic process is:

+ Hand-code a small set of documents (say 1000) for whatever variable(s) you care about

+ Train a statistical learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors

+ Evaluate the effectiveness of the statistical learning model via cross-validation

+ Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (say 1000000)

## Loading necessary libraries

```{r message=FALSE, warning=FALSE}
library(R.utils)
library(tidyverse)
library(tidytext)
library(readtext)
library(stringr)
library(tm)
library(rpart)
library(rpart.plot)
library(e1071)
library(dplyr)
library(caret)
# Library for parallel processing
library(doMC)
registerDoMC(cores=detectCores())
library(knitr)
```


## Download the dataset into local machine


```{r}
base_url_spam <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"
spam_zip <- "20030228_spam_2.tar.bz2"
spam_tar <- "20030228_spam_2.tar"

base_url_ham <- "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2"
ham_zip <- "20030228_easy_ham_2.tar.bz2"
ham_tar <- "20030228_easy_ham_2.tar"
  if(!file.exists(spam_tar)){
    res_spam <- tryCatch(download.file(base_url_spam,
                                  destfile= spam_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(spam_zip)
    untar(spam_tar, exdir="spam_ham_documents")
  } 

  if(!file.exists(ham_tar)){
    res_ham <- tryCatch(download.file(base_url_ham,
                                  destfile= ham_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(ham_zip)
    untar(ham_tar, exdir = "spam_ham_documents")
    
  } else {
    paste("The file is already exists!")
  }
```

## Read content of each file


```{r}
base_dir <- "/Users/salma_elshahawy/Desktop/MSDS_2019/Fall2019/aquisition_management_607/week_10/spam_ham_documents"

email_content <- NA

get_content <- function(type) {
  files_path <- paste(base_dir,type, sep = "/")
  files_name <- list.files(files_path)
    for (file in 1:length(files_name)) {
      file_path <- paste(files_path, files_name[file], sep = "/")
      content_per_file <- file_path %>%
        lapply(readLines)
      
      email_content <- c(email_content, content_per_file)
    }
  return(email_content)
}
```

```{r}
spam_test <- get_content("spam_2") #list
ham_test <- get_content("easy_ham_2") #list
```


```{r}
get_nested_content <- function(list_name) {
  nested_value <- NA
  for (value in 2:length(list_name)) {
    value_per_row <- list_name[[value]]
    nested_value <- c(nested_value, value_per_row)
  }
  return(nested_value)
}
```

```{r}
spam_content <- get_nested_content(spam_test)
ham_content <- get_nested_content(ham_test)
```

## create a datafram for the nested list

```{r}
spam_df_1 <- as.data.frame(spam_content) %>%
  mutate(class = "spam") %>% #adding a class tag 
  na.omit(spam_df_1)
spam_df <- spam_df_1[c(2:10000), c(1:2)]
names(spam_df) <- c("text", "class")
spam_df
```
```{r}
ham_df_1 <- as.data.frame(ham_content) %>%
  mutate(class = "ham") %>% #adding a class tag 
  na.omit(ham_df_1) 
ham_df <- ham_df_1[c(2:6800), c(1:2)]
names(ham_df) <- c("text", "class")
ham_df
```

```{r}
data_df <- rbind(spam_df, ham_df) %>%
  mutate_all(funs(gsub("[^[:alnum:][:blank:]+\\s+?&-]", "",.)))
data_df 
```

```{r}
table(data_df$class)
```
## Convert the 'class' variable from character to factor.

```{r}
data_df$class <- as.factor(data_df$class)
prop.table(table(data_df$class))
```

## Converting into corpus

Convert the resulting documents from downloading into corpus. To do so, I wrote a function as the following 

```{r}
corpus_data = VCorpus(VectorSource(data_df$text))
inspect(corpus_data[1:3])
```

## cleaning up

```{r}
corpus_data = tm_map(corpus_data, content_transformer(stringi::stri_trans_tolower))
corpus_data = tm_map(corpus_data, removeNumbers)
corpus_data = tm_map(corpus_data, removePunctuation)
corpus_data = tm_map(corpus_data, stripWhitespace)
corpus_data = tm_map(corpus_data, removeWords, stopwords("english"))
corpus_data = tm_map(corpus_data, stemDocument)

#as.character(corpus[[1]])
```

## Creating bag of words using Document tearm matrix
Matrix representation of Bag of Words : The Document Term Matrix
We represent the bag of words tokens with a document term matrix (DTM). The rows of the DTM correspond to documents in the collection, columns correspond to terms, and its elements are the term frequencies. We use a built-in function from the ‘tm’ package to create the DTM.

```{r}
#we need our data in a one-row-per-document format. That is, a document-term matrix.
dtm <- DocumentTermMatrix(corpus_data)
dtm
dim(dtm)
```

```{r}
dtm <- removeSparseTerms(dtm, 0.999)
dim(dtm)
inspect(dtm[40:50, 10:15])
```

```{r}
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
datasetNB <- apply(dtm, 2, convert_count)

dataset = as.data.frame(as.matrix(datasetNB))
```


3.1 Building Word Frequency
We want to words that frequently appeared in the dataset. Due to the number of words in the dataset, we are keeping words that appeared more than 60 times.

```{r}
freq<- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
tail(freq, 10)
```

```{r}
findFreqTerms(dtm, lowfreq=60) #identifying terms that appears frequently

```

```{r}
wf<- data.frame(word=names(freq), freq=freq)
head(wf)
```


```{r}
pp <- ggplot(subset(wf, freq>100), aes(x=reorder(word, -freq), y =freq)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x=element_text(angle=45, hjust=1))
pp
```

```{r}
library("wordcloud")
library("RColorBrewer")
set.seed(1234)
wordcloud(words = wf$word, freq = wf$freq, min.freq = 1,
          max.words=150, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}
dataset$class = data_df$class
str(dataset$class)
```

4.1 Splitting the dataset into the Training set and Test set
The usual practice in Machine Learning is to split the dataset into both training and test set. While the model is built on the training set; the model is evaluated on the test set which the model has not been exposed to before.

In order to ensure that the samples; both train and test, are the true representation of the dataset, we check the proportion of the data split.

```{r}
set.seed(222)
split = sample(2,nrow(dataset),prob = c(0.75,0.25),replace = TRUE)
train_set = dataset[split == 1,]
test_set = dataset[split == 2,] 

prop.table(table(train_set$class))
```

```{r}
prop.table(table(test_set$class))
```


4.2 Model Fitting
We will be building our model on 3 different Machine Learning algorithms which are Random Forest, Naive Bayes and Support Vector Machine for the purpose of deciding which perform the best.

4.2.1 Random Forest Classifier.
The Random Forest Model is an ensemble method of Machine Learning with which 300 decision trees were used to build this model with the mode of the outcomes of each individual trees taken as the final output.


```{r}
library(randomForest)
rf_classifier = randomForest(x = train_set[-1210],
                          y = train_set$class,
                          ntree = 300)

rf_classifier
```
The rf_classifier was able to accurately classify the text messages as ham and spam respectively with the class error of 0 which suggest that there is 100% accuracy of the model on the training set of observations. This is expected as the model was exposed to this set of data. 

## 4.2.1.1 Making Predictions and evaluating the Random Forest Classifier.

We want to evaluate the model using the test_set and see if our model can match the 100% accuracy on this new set of data in comparison to the one obtained from the training set.

```{r}
# Predicting the Test set results
rf_pred = predict(rf_classifier, newdata = test_set[-1210])

# Making the Confusion Matrix
library(caret)
library(lattice)
library(ggplot2)
confusionMatrix(table(rf_pred,test_set$class))
```
The Random Forest Classifier(rf_classifier) performed exceptionally well on this data set as the model accuracy is 0.9986. Again, we need not be too excited as there is the possibility of Random Forest to overfit.

Naive Bayes Classifier is a Machine Learning model that is based upon the assumptions of conditional probability as proposed by Bayes’ Theorem. It is fast and easy.

```{r}
library(e1071)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
system.time( classifier_nb <- naiveBayes(train_set, train_set$class, laplace = 1,
                                         trControl = control,tuneLength = 7) )
```
4.2.2.1 Making Predictions and evaluating the Naive Bayes Classifier.

```{r}
nb_pred = predict(classifier_nb, type = 'class', newdata = test_set)

confusionMatrix(nb_pred,test_set$class)
```
The Naive Bayes Classifier also performed very well on the training set by achieving 0.9995 accuracy which means we have got 2 misclassifications out a possible 1209 observation. While the model has a 100% sensitivity rate; the proportion of the positive class predicted as positive, it was able to achieve about 0.9992 on specificity rate which is the proportion of the negative class predicted accurately i.e 2470 out of 2472.

4.2.3 Support Vector Machine
The Support Vector Machine is another algorithm that finds the hyperplane that differentiates the two classes to be predicted, ham and spam in this case; very well. SVM can perform both linear and non-linear classification problems.

```{r}
svm_classifier <- svm(class~., data=train_set)
svm_classifier
```

Our model employs a total of 10101 support vectors while building this classification model.

4.2.3.1 Making Predictions and evaluating the Support Vector Machine Classifier

```{r}
svm_pred = predict(svm_classifier,test_set)

confusionMatrix(svm_pred,test_set$class)
```

The Support Vector Machine model performed badly on this dataset as the model performed exactly as a mere guess. With the accuracy of 0.6693, we may be tempted to think the performance is good but a closer look at specificity rate of 0.991 suggest our model is not doing good.

Part V Conclusion and Validity
5.1 Explaining the validity of the model

The essence of building a spam classifier is for the model to be able to effectively categorise an incoming email as either spam or ham. A model will not be doing very well if it is unable to categorise both categories effectively. As much as we can expect some element errors in our predictions, we are also expecting our model to do a nice job. The Random Forest and Naive Bayes performed exceptionally well in this project.

5.2 Conclusion.
This spam classifier was built just for academic purposes and as such suggestion on what to improve on or what was not properly done are




