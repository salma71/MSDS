---
title: "DATA607_Project 4_Document classification"
author: "Salma Elshahawy"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
 knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Assignment statement

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). [One example corpus:](https://spamassassin.apache.org/old/publiccorpus/)

## Introduction 

A common task in social science involves hand-labeling sets of documents for specific variables (e.g. manual coding). In previous years, this required hiring a set of research assistants and training them to read and evaluate text by hand. It was expensive, prone to error, required extensive data quality checks, and was infeasible if you had an extremely large corpus of text that required classification.

Alternatively, we can now use statistical learning models to classify text into specific sets of categories. This is known as supervised learning. The basic process is:

+ Hand-code a small set of documents (say 1000) for whatever variable(s) you care about

+ Train a statistical learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors

+ Evaluate the effectiveness of the statistical learning model via cross-validation

+ Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (say 1000000)

## Loading necessary libraries

```{r message=FALSE, warning=FALSE}
library(R.utils)
library(tidyverse)
library(tidytext)
library(readtext)
library(stringr)
library(tm)
library(rpart)
library(rpart.plot)
library(e1071)
library(dplyr)
library(caret)
# Library for parallel processing
library(doMC)
registerDoMC(cores=detectCores())
library(R.cache)
library(knitr)
```


## Download the dataset into local machine


```{r}
base_url_spam <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"
spam_zip <- "20030228_spam_2.tar.bz2"
spam_tar <- "20030228_spam_2.tar"

base_url_ham <- "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2"
ham_zip <- "20030228_easy_ham_2.tar.bz2"
ham_tar <- "20030228_easy_ham_2.tar"
  if(!file.exists(spam_tar)){
    res_spam <- tryCatch(download.file(base_url_spam,
                                  destfile= spam_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(spam_zip)
    untar(spam_tar, exdir="spam_ham_documents")
  } 

  if(!file.exists(ham_tar)){
    res_ham <- tryCatch(download.file(base_url_ham,
                                  destfile= ham_folder,
                                  method="curl"),
                error=function(e) 1)
    bunzip2(ham_zip)
    untar(ham_tar, exdir = "spam_ham_documents")
    
  } else {
    paste("The file is already exists!")
  }
clean_cache(clean = TRUE)
```

## Read content of each file


```{r}
base_dir <- "/Users/salmaelshahawy/Desktop/MSDS_2019/Fall2019/aquisition_management_607/week_10/spam_ham_documents"

email_content <- NA

get_content <- function(type) {
  files_path <- paste(base_dir,type, sep = "/")
  files_name <- list.files(files_path)
    for (file in 1:length(files_name)) {
      file_path <- paste(files_path, files_name[file], sep = "/")
      content_per_file <- file_path %>%
        lapply(readLines)
      
      email_content <- c(email_content, content_per_file)
    }
  return(email_content)
}
```

```{r}
spam_test <- get_content("spam_2") #list
ham_test <- get_content("easy_ham_2") #list
```


```{r}
get_nested_content <- function(list_name) {
  nested_value <- NA
  for (value in 2:length(list_name)) {
    value_per_row <- list_name[[value]]
    nested_value <- c(nested_value, value_per_row)
  }
  return(nested_value)
}
```

```{r}
spam_content <- get_nested_content(spam_test)
ham_content <- get_nested_content(ham_test)
```

## create a datafram for the nested list

```{r}
spam_df <- as.data.frame(spam_content) %>%
  mutate(class = "spam") %>% #adding a class tag 
  na.omit(spam_df)
names(spam_df) <- c("text", "class")
spam_df
```
```{r}
ham_df <- as.data.frame(ham_content) %>%
  mutate(class = "ham") %>% #adding a class tag 
  na.omit(ham_df) 
names(ham_df) <- c("text", "class")
ham_df
```

```{r}
data_df <- rbind(spam_df, ham_df) %>%
  mutate_all(funs(gsub("[^[:alnum:][:blank:]+\\s+?&-]", "",.)))
data_df 
```

```{r}
table(data_df$class)
```
## Convert the 'class' variable from character to factor.

```{r}
data_df$class <- as.factor(data_df$class)
```



to_corpus <- function(df, type) {
  corpus_name <- VCorpus(VectorSource(df)) %>%
    add_tag(tag = "document type", value = type)
  return(corpus_name)
}


add_tag <- function(corpus, tag, value){
  for (i in 1:length(corpus)){
    meta(corpus[[i]], tag) <- value                    # Add the value to the specified tag
  }
  return(corpus)
}


## Converting into corpus

Convert the resulting documents from downloading into corpus. To do so, I wrote a function as the following 

```{r}
corpus_data = VCorpus(VectorSource(data_df$text))
inspect(corpus_data[1:3])
```

## cleaning up

```{r}
corpus_data = tm_map(corpus_data, content_transformer(stringi::stri_trans_tolower))
corpus_data = tm_map(corpus_data, removeNumbers)
corpus_data = tm_map(corpus_data, removePunctuation)
corpus_data = tm_map(corpus_data, stripWhitespace)
corpus_data = tm_map(corpus_data, removeWords, stopwords("english"))
corpus_data = tm_map(corpus_data, stemDocument)

#as.character(corpus[[1]])
```

## Creating bag of words using Document tearm matrix
Matrix representation of Bag of Words : The Document Term Matrix
We represent the bag of words tokens with a document term matrix (DTM). The rows of the DTM correspond to documents in the collection, columns correspond to terms, and its elements are the term frequencies. We use a built-in function from the ‘tm’ package to create the DTM.

```{r}
#we need our data in a one-row-per-document format. That is, a document-term matrix.
dtm <- DocumentTermMatrix(corpus_data)
dtm
```

## Partitioning the Data
Next, we create 75:25 partitions of the dataframe, corpus and document term matrix for training and testing purposes.
```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(data_df))

## set the seed to make your partition reproducible
set.seed(200)
train_ind <- sample(seq_len(nrow(data_df)), size = smp_size)

df.train <- data_df[train_ind, ]
df.test <- data_df[-train_ind, ]

dtm.train <- dtm[train_ind, ]
dtm.test <- dtm[-train_ind, ]

corpus_data.train <- corpus_data[1: 250000]
corpus_data.test <- corpus_data[251000:331773]

```

```{r}
dim(dtm.train)
```

The DTM contains 38957 features but not all of them will be useful for classification. We reduce the number of features by ignoring words which appear in less than five reviews. To do this, we use ‘findFreqTerms’ function to indentify the frequent words, we then restrict the DTM to use only the frequent words using the ‘dictionary’ option.

{r}
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
datasetNB <- apply(dtm, 2, convert_count)

dataset = as.data.frame(as.matrix(datasetNB))


```{r}
fivefreq <- findFreqTerms(dtm.train, 10)
length((fivefreq))
## [1] 12144

# Use only 10 most frequent words (fivefreq) to build the DTM

dtm.train.nb <- DocumentTermMatrix(corpus_data.train, control=list(dictionary = fivefreq))

dim(dtm.train.nb)
## [1]  1500 12144

dtm.test.nb <- DocumentTermMatrix(corpus_data.test, control=list(dictionary = fivefreq))

dim(dtm.train.nb)
```

## The Naive Bayes algorithm

```{r}
# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}
```

## Boolean feature Multinomial Naive Bayes

```{r}
# Apply the convert_count function to get final training and testing DTMs
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)
```

## Training the Naive Bayes Model
To train the model we use the naiveBayes function from the ‘e1071’ package. Since Naive Bayes evaluates products of probabilities, we need some way of assigning non-zero probabilities to words which do not occur in the sample. We use Laplace 1 smoothing to this end.

```{r}
# Train the classifier
system.time( classifier <- naiveBayes(trainNB, df.train$class, laplace = 1) )
```

```{r}
# Use the NB classifier we built to make predictions on the test set.
system.time( pred <- predict(classifier, newdata=testNB) )
```

```{r}
# Create a truth table by tabulating the predicted class labels with the actual class labels 
table("Predictions"= pred,  "Actual" = df.test$class )
```







```{r}
dtm = removeSparseTerms(dtm, 0.999)

dtm 
```

```{r}
freq<- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
tail(freq, 10)
```
```{r}
findFreqTerms(dtm, lowfreq=100)
```

```{r}
wf<- data.frame(word=names(freq), freq=freq)
head(wf)
mean(wf$freq)
pp <- ggplot(subset(wf, freq>1200), aes(x=reorder(word, -freq), y =freq)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x=element_text(angle=45, hjust=1))
pp
```

```{r}
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
datasetNB <- apply(dtm, 2, convert_count)

dataset = as.data.frame(as.matrix(datasetNB))
```

To obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents.
```{r}
# Remove sparse terms (that don't appear very often)
spdtm = removeSparseTerms(dtm, 0.99)
spdtm
```

```{r}
emailsSparse = as.data.frame(as.matrix(spdtm))
# make variable names of emailsSparse valid i.e. R-friendly (to convert variables names starting with numbers)

colnames(emailsSparse) = make.names(colnames(emailsSparse))
# word stem that shows up most frequently across all the emails:
sort(colSums(emailsSparse))
# Add dependent variable to this dataset
emailsSparse$spam = email$spam
# most frequent words in ham:
sort(colSums(subset(emailsSparse, spam == 0)))
```

```{r}
emailsSparse$spam = as.factor(emailsSparse$spam)
library(caTools)
set.seed(123)
spl <- sample.split(emailsSparse$spam, 0.7)
train <- subset(emailsSparse, spl == TRUE)
test <- subset(emailsSparse, spl == FALSE)

# Build a logistic regression model
spamLog <- glm(spam~., data=train, family="binomial")
```



## Create a lable for spam/ham document category label

```{r}
add_tag <- function(corpus, tag, value){
  for (i in 1:length(corpus)){
    meta(corpus[[i]], tag) <- value                    # Add the value to the specified tag
  }
  return(corpus)
}
```



## Clean the corpus using **tm** package

```{r}
get_clean <- function(corpus_name){
  cleaned_corpus <- corpus_name %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(content_transformer(stringi::stri_trans_tolower)) %>%
    tm_map(PlainTextDocument)
  
  return(cleaned_corpus)
}
```

```{r} 
cleaned_spam <- get_clean(spam_corpus)
```




```{r}
# for the spam folder
spam_corpus <- VCorpus(DirSource(spam_df))
#summary(spam_corpus)
```
##############

# for the ham folder
ham_files <- list.files("././spam_ham_documents/easy_ham_2")
ham_path <- file.path("././spam_ham_documents/easy_ham_2")
ham_corpus <- VCorpus(DirSource(ham_path))
#summary(spam_corpus)

```

## Cleaning up the corpus using the tm package

For this purpose, I constructed a function that takes the corpus as an argument and use **tm** package from R to clean up the corpus.
