---
title: "DATA607_Project 4_Document classification"
author: "Salma Elshahawy"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment statement

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). [One example corpus:](https://spamassassin.apache.org/old/publiccorpus/)

## Introduction 

A common task in social science involves hand-labeling sets of documents for specific variables (e.g. manual coding). In previous years, this required hiring a set of research assistants and training them to read and evaluate text by hand. It was expensive, prone to error, required extensive data quality checks, and was infeasible if you had an extremely large corpus of text that required classification.

Alternatively, we can now use statistical learning models to classify text into specific sets of categories. This is known as supervised learning. The basic process is:

+ Hand-code a small set of documents (say 1000) for whatever variable(s) you care about

+ Train a statistical learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors

+ Evaluate the effectiveness of the statistical learning model via cross-validation

+ Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (say 1000000)

## Loading necessary libraries

```{r message=FALSE, warning=FALSE}
library(R.utils)
library(tidyverse)
library(tidytext)
library(readtext)
library(stringr)
library(tm)
```

## Download the dataset into local machine


```{r}
base_url_spam <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"
spam_zip <- "20030228_spam_2.tar.bz2"
spam_tar <- "20030228_spam_2.tar"

base_url_ham <- "https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2"
ham_zip <- "20030228_easy_ham_2.tar.bz2"
ham_tar <- "20030228_easy_ham_2.tar"
  if(!file.exists(spam_tar)){
    res_spam <- tryCatch(download.file(base_url_spam,
                                  destfile= spam_folder,
                                  method="auto"),
                error=function(e) 1)
    bunzip2(spam_zip)
    untar(spam_tar, exdir="spam_ham_documents")
    
  } 

  if(!file.exists(ham_tar)){
    res_ham <- tryCatch(download.file(base_url_ham,
                                  destfile= ham_folder,
                                  method="auto"),
                error=function(e) 1)
    bunzip2(ham_zip)
    untar(ham_tar, exdir = "spam_ham_documents")
    
  } else {
    paste("The file is already exists!")
  }
```

## Read content of each file

```{r}
get_email_body <- function(email_content){
  msge <- str_split(email_content,"\n\n") %>% unlist()
  body <- paste(msge[2:length(msge)], collapse=' ' )
  return(body)
}
```

```{r}
base_dir <- "/Users/salmaelshahawy/Desktop/MSDS_2019/Fall2019/aquisition_management_607/week_10/spam_ham_documents"

email_content <- NA

get_content <- function(type) {
  files_path <- paste(base_dir,type, sep = "/")
  files_name <- list.files(files_path)
    for (file in 1:length(files_name)) {
      file_path <- paste(files_path, files_name[file], sep = "/")
      content_per_file <- file_path %>%
        lapply(readLines)
      email_content <- c(email_content, content_per_file)
    }
  return(email_content)
}
```

```{r}
spam_test <- get_content("spam_2")
ham_test <- get_content("easy_ham_2")
```

## Converting into corpus

Convert the resulting documents from downloading into corpus. To do so, I wrote a function as the following 

```{r}
to_corpus <- function(doc) {
  corpus_name <- doc %>%
    VectorSource() %>%
    VCorpus() 
  
  cleaned_corpus <- corpus_name %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(tolower) %>%
    tm_map(PlainTextDocument) %>%
    tm_map(removeWords, stopwords("en")) %>%
    tm_map(stripWhitespace)
  return(cleaned_corpus)
}
```

```{r}
spam_corpus <- to_corpus(spam_test)
summary(spam_corpus)
```

## Convert the results into a dataframe

```{r}
make_df <- function(content) {
  df <- data.frame(matrix(unlist(content), 
                          nrow=length(content), 
                          byrow=T),
                   stringsAsFactors=FALSE)
  return(df)
}

spam_df <- make_df(spam_test)
ham_df <- make_df(ham_test)
View(spam_df)
View(ham_df)
```

```{r}
# adding tag to each document either spam or ham
get_class <- function(corpus, value){
  for (doc in 1:length(corpus)){
      if(value == "spam"){
      meta(corpus[[doc]]) <- "spam"                
      } else {
      meta(corpus[[doc]]) <- "ham"
    }
  }
  return(corpus)
}

```



```{r}
# for the spam folder
spam_corpus <- VCorpus(DirSource(spam_df))
#summary(spam_corpus)
```
##############

# for the ham folder
ham_files <- list.files("././spam_ham_documents/easy_ham_2")
ham_path <- file.path("././spam_ham_documents/easy_ham_2")
ham_corpus <- VCorpus(DirSource(ham_path))
#summary(spam_corpus)

```

## Cleaning up the corpus using the tm package

For this purpose, I constructed a function that takes the corpus as an argument and use **tm** package from R to clean up the corpus.

```{r}
readContent <- function(content){
  msge <- str_split(content,"\n\n") %>% unlist()
  body <- paste(msge[2:length(msge)], collapse=' ' )
  return(body)
}

```

```{r}
dir <- "/Users/salmaelshahawy/Desktop/MSDS_2019/Fall2019/aquisition_management_607/week_10/spam_ham_documents/spam_2/"

files <- list.files(dir)
content <- NA
for (file in 1:length(files)) {
  file_path <- paste0(dir, files[1])
  email <- readtext(file_path)
  msg <- readContent(email)
  msg <- gsub("<.*?>", " ", msg)
  each_msg <- list(paste(msg,collapse = "\n"))
  content = c(content, each_msg)
}
```


```{r}
cleanDoc <- function(corpus) {
    corpus <- corpus %>%
    tm_map(removeNumbers) %>%                       # Remove numbers
    tm_map(removePunctuation) %>%                   # Remove punctuation symbols 
    tm_map(tolower) %>%                             # Transform  to lowercase
    tm_map(PlainTextDocument) %>%                   # Transform back to PlainTextDocument
    tm_map(removeWords, stopwords("en")) %>%        # Remove stopwords
    tm_map(stripWhitespace) %>%                     # Remove white spaces
    tm_map(stemDocument)                            # Reduce to stems
    
  return(corpus)
}
```

```{r}

addTag <- function(corpus, tag, value){
  for (i in 1:length(corpus)){
    meta(corpus[[i]], tag) <- value                    # Add the value to the specified tag
  }
  return(corpus)
}
```


```{r}
# Create spam corpus
spam_corpus <- spam_2%>%
   toCorpus %>% 
   cleanDoc  %>% 
   addTag(tag = "spam_ham", value = "spam")



# Create ham corpus
ham_corpus <- easy_ham_2 %>%
  toCorpus %>%
  cleanDoc %>%
  addTag(tag = "spam_ham", value = "ham")
```






